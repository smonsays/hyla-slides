<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Simon Schug">
  <title>Attention as a hypernetwork</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="./reveal.js/dist/reset.css">
  <link rel="stylesheet" href="./reveal.js/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="./reveal.js/dist/theme/robolung.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Attention as a hypernetwork</h1>
  <p class="author">Simon Schug</p>
  <p class="date">ml-next • 2024/09/24</p>
</section>

<section id="section" class="slide level2">
<h2></h2>
<p><a href="https://arxiv.org/abs/2406.05816"><img src="assets/abstract.svg" style="width:70%;height:70%;"></a></p>
</section>
<section id="compositionality" class="slide level2">
<h2>Compositionality</h2>
<ul>
<li>The world is compositional or at least our model of it is
<ul>
<li><a href="">Vision</a>: Scenes made from objects, objects made from
parts</li>
<li><a href="">Language</a>: Compositions of words according to
grammatical rules</li>
<li><a href="">Tasks</a>: Complex behaviours can be decomposed into
simpler skills</li>
</ul></li>
</ul>
<p><img src="assets/compositionality.svg" style="width:70%;height:70%;"></p>
<blockquote>
<p>Compositionality fuels a <strong>combinatorial explosion</strong>.
How do <s>neural networks</s> transformers generalize to compositions
not seen during training?</p>
</blockquote>
<!-- - Why use multiple heads in the attention mechanism? Why not use more depth/width instead? -->
</section>
<section>
<section id="multi-head-attention-is-a-hypernetwork"
class="title-slide slide level1" data-background="#08407E">
<h1 data-background="#08407E"><font color="#ffffff"> Multi-head
attention is a hypernetwork </font></h1>

</section>
<section id="hypernetwork" class="slide level2">
<h2>Hypernetwork</h2>
<p>A hypernetwork is a neural network that reconfigures <br> the
computation of another neural network.</p>
<p><img src="assets/hypernetwork.svg" style="width:60%;height:60%;"></p>
</section>
<section id="multi-head-attention" class="slide level2">
<h2>Multi-head attention</h2>
<blockquote>
<p>Multi-head attention is a linear hypernetwork with a linear value
network.</p>
</blockquote>
<p>Given the attention scores <span
class="math inline">\(\mathbf{\mathcal{A}} = \sigma \bigl ( \;
\left[\mathbf{\mathrm{\tilde{A}}}_1 \; \mathbf{\mathrm{\tilde{A}}}_2 \;
\ldots \; \mathbf{\mathrm{\tilde{A}}}_H \right] \; \bigr ) \text{ with }
\mathbf{\mathrm{\tilde{A}}}_h = \frac{\mathbf{\mathrm{Q}}_h
\mathbf{\mathrm{K}}_h^\top}{\sqrt{D^{\text{key}}}}\)</span> <br> and
<span
class="math inline">\(\sigma(\cdot)=\mathrm{Softmax}(\cdot)\)</span>,
multi-head attention can be reformulated as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\text{MHA}_{q}(\mathbf{\mathrm{X}})
:=&amp; \mathbf{W}^{\text{out}} \bigoplus_{h=1}^H \sum_{k=1}^T a_{h,q,k}
\; \mathbf{W}^{\text{value}}_h \mathbf{x}_k \\
=&amp; \sum_{h=1}^H \mathbf{W}^{\text{out}}_h \sum_{k=1}^T a_{h,q,k} \;
\mathbf{W}^{\text{value}}_h \mathbf{x}_k \\
=&amp; \sum_{k=1}^T \left(\sum_{h=1}^H
\underbrace{a_{h,q,k}}_{\text{latent code}} \;
\underbrace{\mathbf{W}^{\text{out}}_h
\mathbf{W}^{\text{value}}_h}_{\text{hypernetwork}} \right)
\mathbf{x}_k  \\
=&amp; \sum_{k=1}^T \underbrace{\mathbf{\mathrm{W}}_{q,k}}_{\text{value
network}} \mathbf{x}_k
\end{aligned}
\]</span></p>
</section>
<section id="multi-head-attention-1" class="slide level2">
<h2>Multi-head attention</h2>
<blockquote>
<p>Multi-head attention can be viewed as a linear hypernetwork with a
linear value network.</p>
</blockquote>
<p><img src="assets/mha.svg" style="width:65%;height:65%;"></p>
</section>
<section id="attention-as-a-hypernetwork" class="slide level2">
<h2>Attention as a hypernetwork</h2>
<p>What are the implications of this perspective?</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The attention scores along the head index represent a <strong>latent
code</strong> that specifies the <em>operation</em> applied to each
key-query pair</li>
<li>Through these latent codes, operations could be shared across known
tasks and reused for unknown tasks</li>
<li>Making the value network more <em>expressive</em> could strengthen
the ability to reuse operations</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img src="assets/attention-scores.svg" style="width:80%;height:80%;">
<!-- ![](assets/raven.png) --></p>
</div>
</div>
</section></section>
<section>
<section id="hypernetwork-linear-attention"
class="title-slide slide level1" data-background="#08407E">
<h1 data-background="#08407E"><font color="#ffffff"> Hypernetwork linear
attention </font></h1>

</section>
<section id="hypernetwork-linear-attention-hyla" class="slide level2">
<h2>Hypernetwork linear attention (HYLA)</h2>
<p>We reinforce the hypernetwork mechanism of multi-head attention by
<br> making the <strong>value network nonlinear</strong>.</p>
<ul>
<li>We do not introduce additional parameters</li>
<li>We normalize attention scores across the head indices using <span
class="math inline">\(\sigma(\cdot)=\mathrm{RMSNorm}(\cdot)\)</span></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\text{HYLA}_{q}(\mathbf{\mathrm{X}})
=&amp; \sum_{k=1}^T \left(\sum_{h=1}^H a_{h,q,k} \;
\mathbf{W}^{\text{out}}_h  \right) \phi \left ( \sum_{h=1}^H a_{h,q,k}
\; \mathbf{W}^{\text{value}}_h \mathbf{x}_k \right ) \\
=&amp; \sum_{k=1}^T \mathbf{\mathrm{W}}&#39;_{q,k} \phi
(\mathbf{\mathrm{W}}_{q,k} \mathbf{x}_k)
\end{aligned}
\]</span></p>
<!-- $$
\text{RMSNorm}(\mathbf{x})=\frac{\mathbf{x}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}}}
$$ -->
</section>
<section id="comparison-to-standard-mha" class="slide level2">
<h2>Comparison to standard MHA</h2>
<p><img src="assets/algorithm.svg" style="width:80%;height:80%;"></p>
</section></section>
<section>
<section id="symbolic-raven-matrices" class="title-slide slide level1"
data-background="#08407E">
<h1 data-background="#08407E"><font color="#ffffff"> Symbolic Raven
matrices </font></h1>

</section>
<section id="raven-progressive-matrices" class="slide level2">
<h2>Raven progressive matrices</h2>
<p>A non-verbal test to measure general human intelligence and abstract
reasoning.<a href="#/fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> <a href="#/fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><img src="assets/carpenter.png" style="width:40%;height:40%;"></p>
<aside class="notes">
<p>Subjects initially assume that the rectangles correspond to each
other, the dark curves correspond to each other and the straight lines
correspond to each other. But to solve the problem, subjects must
backtrack and try other possible bases for correspondence, such as
numerosity or orientation.</p>
</aside>
<aside id="footnotes" class="footnotes footnotes-end-of-section"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Carpenter et al. (1990). What one intelligence test
measures: a theoretical account of the processing in the Raven
Progressive Matrices Test.<a href="#/fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Raven (1962). Advanced Progressive Matrices.<a
href="#/fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="symbolic-raven-sraven" class="slide level2">
<h2>Symbolic Raven (sraven)</h2>
<p>A Raven-inspired benchmark to measure compositional generalization
for abstract reasoning.</p>
<p><img src="assets/sraven_task_a.svg" style="width:95%;height:95%;"></p>
</section>
<section id="finding-correspondences" class="slide level2">
<h2>Finding correspondences</h2>
<p>We model one of the key difficulties of <em>finding
correspondences</em> by applying a fixed permutation to each column.</p>
<p><img src="assets/sraven_task_b.svg" style="width:95%;height:95%;"></p>
</section>
<section id="compositional-generalization" class="slide level2">
<h2>Compositional generalization</h2>
<p>Strengthening the hypernetwork mechanism improves compositional
generalization.</p>
<p><img src="assets/sraven_scaling.svg" style="width:50%;height:50%;"></p>
</section>
<section id="compositional-generalization-ii" class="slide level2">
<h2>Compositional generalization II</h2>
<p>Weakening the hypernetwork mechanism disrupts compositional
generalization.</p>
<p><img src="assets/sraven_heads_ood.svg" style="width:40%;height:40%;"></p>
</section>
<section id="latent-code-structure-i" class="slide level2">
<h2>Latent code structure I</h2>
<p>The hypernetwork latent code is highly structured.</p>
<p><img src="assets/sraven_attention.svg" style="width:70%;height:70%;"></p>
</section>
<section id="latent-code-structure-ii" class="slide level2">
<h2>Latent code structure II</h2>
<p>We can decode the operation a layer implements on OOD tasks.</p>
<p><img src="assets/sraven_decode.svg" style="width:40%;height:40%;"></p>
</section>
<section id="model-ablations" class="slide level2">
<h2>Model ablations</h2>
<p>Is it really the hypernetwork mechanism of HYLA that improves
performance?</p>
<p><img src="assets/ablations.svg" style="width:60%;height:60%;"></p>
</section></section>
<section>
<section id="whats-next" class="title-slide slide level1"
data-background="#08407E">
<h1 data-background="#08407E"><font color="#ffffff"> What’s next?
</font></h1>

</section>
<section id="language-modelling" class="slide level2">
<h2>Language modelling</h2>
<p>HYLA works comparably well on autoregressive language modelling, <br>
despite being a linear attention method (50M parameters, 130B
tokens).</p>
<p><img src="assets/c4.svg" style="width:40%;height:40%;"></p>
<aside class="notes">
<pre><code>cfg.model = (
  D=512,  # model/embed dim  = qkv dim
  H=8,  # num attention heads
  L=512,  # max context/sequence length (move out of config?)
  N=6,  # number of transformer block layers
  F=2048,  # FF inner dimension
  dtype=&quot;bfloat16&quot;,  # computation dtype.
)</code></pre>
</aside>
</section>
<section id="open-questions" class="slide level2">
<h2>Open questions</h2>
<ul>
<li>Can we understand transformer-based foundation models from this
perspective?</li>
<li>Should we be using more hyla? Combine it with softmax
attention?</li>
</ul>
</section></section>
    </div>
  </div>

  <script src="./reveal.js/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="./reveal.js/plugin/notes/notes.js"></script>
  <script src="./reveal.js/plugin/search/search.js"></script>
  <script src="./reveal.js/plugin/zoom/zoom.js"></script>
  <script src="./reveal.js/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: true,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: true,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 960,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
